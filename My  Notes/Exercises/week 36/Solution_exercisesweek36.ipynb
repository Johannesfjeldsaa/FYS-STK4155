{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b2fbbe",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html exercisesweek36.do.txt  -->\n",
    "<!-- dom:TITLE: Exercises week 36 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('Solarize_Light2')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model as skl\n",
    "from sklearn.metrics import mean_squared_error"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T08:07:53.181946900Z",
     "start_time": "2023-09-06T08:07:53.053372100Z"
    }
   },
   "id": "f2fef92246dee3cf"
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [],
   "source": [
    "# Where to save the figures and data files\n",
    "PROJECT_ROOT_DIR = \"Results\"\n",
    "FIGURE_ID = \"Results/FigureFiles\"\n",
    "DATA_ID = \"DataFiles/\"\n",
    "\n",
    "if not os.path.exists(PROJECT_ROOT_DIR):\n",
    "    os.mkdir(PROJECT_ROOT_DIR)\n",
    "\n",
    "if not os.path.exists(FIGURE_ID):\n",
    "    os.makedirs(FIGURE_ID)\n",
    "\n",
    "if not os.path.exists(DATA_ID):\n",
    "    os.makedirs(DATA_ID)\n",
    "\n",
    "def image_path(fig_id):\n",
    "    return os.path.join(FIGURE_ID, fig_id)\n",
    "\n",
    "def data_path(dat_id):\n",
    "    return os.path.join(DATA_ID, dat_id)\n",
    "\n",
    "def save_fig(fig_id):\n",
    "    plt.savefig(image_path(fig_id) + \".png\", format='png')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T08:07:53.234313200Z",
     "start_time": "2023-09-06T08:07:53.062627100Z"
    }
   },
   "id": "52cd3f78c4f44bab"
  },
  {
   "cell_type": "markdown",
   "id": "da488c0c",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Exercises week 36\n",
    "**September 4-8, 2023**\n",
    "\n",
    "Date: **Deadline is Sunday September 10 at midnight**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84d6b13",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Overarching aims of the exercises this week\n",
    "\n",
    "This set of exercises form an important part of the first project. The\n",
    "analytical exercises deal with the material covered last week on the\n",
    "mathematical interpretations of ordinary least squares and of Ridge\n",
    "regression. The numerical exercises can be seen as a continuation of\n",
    "exercise 3 from week 35, with the inclusion of Ridge regression. This\n",
    "material enters also the discussions of the first project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c9c28e",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 1: Analytical exercises\n",
    "\n",
    "The aim here is to derive the expression for the optimal parameters\n",
    "using Ridge regression. Furthermore, using the singular value\n",
    "decomposition, we will analyze the difference between the ordinary\n",
    "least squares approach and Ridge regression.\n",
    "\n",
    "The expression for the standard Mean Squared Error (MSE) which we used to define our cost function and the equations for the ordinary least squares (OLS) method, was given by the\n",
    "optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439f1456",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\beta}\\in {\\mathbb{R}}^{p}}}\\frac{1}{n}\\left\\{\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\right)^T\\left(\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\right)\\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e09f7",
   "metadata": {
    "editable": true
   },
   "source": [
    "which we can also write as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c45981",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\beta}\\in\n",
    "{\\mathbb{R}}^{p}}}\\frac{1}{n}\\sum_{i=0}^{n-1}\\left(y_i-\\tilde{y}_i\\right)^2=\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\vert\\vert_2^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0e91ea",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have used the definition of  a norm-2 vector, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5805f35",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\vert\\vert \\boldsymbol{x}\\vert\\vert_2 = \\sqrt{\\sum_i x_i^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3095bf",
   "metadata": {
    "editable": true
   },
   "source": [
    "By minimizing the above equation with respect to the parameters\n",
    "$\\boldsymbol{\\beta}$ we could then obtain an analytical expression for the\n",
    "parameters $\\boldsymbol{\\beta}$.\n",
    "\n",
    "We can add a regularization parameter $\\lambda$ by\n",
    "defining a new cost function to be optimized, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da90fe04",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "{\\displaystyle \\min_{\\boldsymbol{\\beta}\\in\n",
    "{\\mathbb{R}}^{p}}}\\frac{1}{n}\\vert\\vert \\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{\\beta}\\vert\\vert_2^2+\\lambda\\vert\\vert \\boldsymbol{\\beta}\\vert\\vert_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a106e07",
   "metadata": {
    "editable": true
   },
   "source": [
    "which leads to the Ridge regression minimization problem. One can require as part of the optimization problem \n",
    "that $\\vert\\vert \\boldsymbol{\\beta}\\vert\\vert_2^2\\le t$, where $t$ is\n",
    "a finite number larger than zero. We will not implement that here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3917877b",
   "metadata": {
    "editable": true
   },
   "source": [
    "### a) Expression for Ridge regression\n",
    "\n",
    "Show that the optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78226f28",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{\\mathrm{Ridge}} = \\left(\\boldsymbol{X}^T\\boldsymbol{X}+\\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951dfffa",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $\\boldsymbol{I}$ being a $p\\times p$ identity matrix with the constraint that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d2770e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\sum_{i=0}^{p-1} \\beta_i^2 \\leq t,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec212498",
   "metadata": {
    "editable": true
   },
   "source": [
    "with $t$ a finite positive number. \n",
    "\n",
    "The ordinary least squares result is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffabf6c",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}} = \\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a6f45",
   "metadata": {
    "editable": true
   },
   "source": [
    "### b) The singular value decomposition\n",
    "\n",
    "Use the singular value decomposition of an n\\times p$ matrix $\\boldsymbol{X}$ (our design matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761ed23",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\boldsymbol{X}=\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8479e",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\boldsymbol{U}$ and $\\boldsymbol{V}$ are orthogonal matrices of dimensions\n",
    "$n\\times n$ and $p\\times p$, respectively, and $\\boldsymbol{\\Sigma}$ is an\n",
    "$n\\times p$ matrix which contains the ingular values only. This material was discussed during the lectures of week 35.\n",
    "\n",
    "Show that you can write the \n",
    "OLS solutions in terms of the eigenvectors (the columns) of the orthogonal matrix  $\\boldsymbol{U}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df91bda",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\boldsymbol{y}}_{\\mathrm{OLS}}=\\boldsymbol{X}\\boldsymbol{\\beta}  = \\sum_{j=0}^{p-1}\\boldsymbol{u}_j\\boldsymbol{u}_j^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6de0312",
   "metadata": {
    "editable": true
   },
   "source": [
    "For Ridge regression, show that the corresponding equation is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e09d132",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\tilde{\\boldsymbol{y}}_{\\mathrm{Ridge}}=\\boldsymbol{X}\\boldsymbol{\\beta}_{\\mathrm{Ridge}} = \\boldsymbol{U\\Sigma V^T}\\left(\\boldsymbol{V}\\boldsymbol{\\Sigma}^2\\boldsymbol{V}^T+\\lambda\\boldsymbol{I} \\right)^{-1}(\\boldsymbol{U\\Sigma V^T})^T\\boldsymbol{y}=\\sum_{j=0}^{p-1}\\boldsymbol{u}_j\\boldsymbol{u}_j^T\\frac{\\sigma_j^2}{\\sigma_j^2+\\lambda}\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c924ab",
   "metadata": {
    "editable": true
   },
   "source": [
    "with the vectors $\\boldsymbol{u}_j$ being the columns of $\\boldsymbol{U}$ from the SVD of the matrix $\\boldsymbol{X}$. \n",
    "\n",
    "Give an interpretation of the results.  [Section 3.4 of Hastie et al's textbook gives a good discussion of the above results](https://link.springer.com/book/10.1007/978-0-387-84858-7)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9328a1",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 2: Adding Ridge Regression\n",
    "\n",
    "This exercise is a continuation of exercise 3 from week 35, see <https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/exercisesweek35.html>. We will use the same function to generate our data set, still staying with a simple function $y(x)$ which we want to fit using linear regression, but now extending the analysis to include the Ridge regression method.\n",
    "\n",
    "In this exercise you need to include the same elements from last week, that is \n",
    "1. scale your data by subtracting the mean value from each column in the design matrix.\n",
    "\n",
    "2. perform a split of the data in a training set and a test set.\n",
    "\n",
    "The addition to the analysis this time is the introduction of the hyperparameter $\\lambda$ when introducing Ridge regression.\n",
    "\n",
    "Extend the code from exercise 3 from [week 35](https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/exercisesweek35.html) to include Ridge regression with the hyperparameter $\\lambda$. The optimal parameters $\\hat{\\beta}$ for Ridge regression can be obtained by matrix inversion in a similar way as done for ordinary least squares. You need to add to your code the following equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b54b7b4",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{\\mathrm{Ridge}} = \\left(\\boldsymbol{X}^T\\boldsymbol{X}+\\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ebe31b",
   "metadata": {
    "editable": true
   },
   "source": [
    "The ordinary least squares result you encoded last week is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874e0dd3",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}} = \\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{y},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adcc39f",
   "metadata": {
    "editable": true
   },
   "source": [
    "Use these results to compute the mean squared error for ordinary least squares and Ridge regression first for a polynomial of degree five with $n=100$ data points and five selected values of $\\lambda=[0.0001,0.001, 0.01,0.1,1.0]$. Compute thereafter the mean squared error for the same values of $\\lambda$ for polynomials of degree ten and $15$. Discuss your results for the training MSE and test MSE with Ridge regression and ordinary least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "outputs": [],
   "source": [
    "def preprocess(X, y, test_size):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size) # Deler opp modeldataen i trenings og test-data\n",
    "    \n",
    "    #Model training, we compute the mean value of y and X\n",
    "    y_train_mean = np.mean(y_train)\n",
    "    X_train_mean = np.mean(X_train,axis=0)\n",
    "    X_train = X_train - X_train_mean\n",
    "    y_train = y_train - y_train_mean\n",
    "    \n",
    "    #Model prediction, we need also to transform our data set used for the prediction.\n",
    "    X_test = X_test - X_train_mean #Use mean from training data\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, X_train_mean, y_train_mean"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T08:40:05.373925700Z",
     "start_time": "2023-09-08T08:40:05.349257500Z"
    }
   },
   "id": "85e22d7191982d50"
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "outputs": [],
   "source": [
    "# OLS \n",
    "def regression_and_cost_calc(X, y, methode=None, la=None):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, X_train_mean, y_train_mean = preprocess(X, y, test_size=.4)\n",
    "    \n",
    "    \n",
    "    if methode is None or 0:\n",
    "        beta = np.linalg.inv(X_train.T @ X_train) @ (X_train.T @ y_train)\n",
    "    elif methode == 1:\n",
    "        # Ridge regression\n",
    "        if la is None or not np.isreal(la):\n",
    "            raise ValueError('To apply ridge regression provide a numerical values for lambda.')\n",
    "        \n",
    "        XtX = X_train.T @ X_train\n",
    "        beta = np.linalg.inv(XtX + la*np.identity(np.shape(XtX)[0])) @ (X_train.T @ y_train)\n",
    "    \n",
    "    y_tilde = (X_train @ beta) # y_train og x_train er skalert, siden y_tilde kun brukes videre til MSE_train med y_train trenger ikke denne skalering \n",
    "    y_predicted = (X_test @ beta) + y_train_mean # y_predicted skal brukes til MSE_test med y_test som IKKE er skalert, vi må derfor skalere y_predicted tilbake.\n",
    "    \n",
    "    MSE_train = mean_squared_error(y_train, y_tilde)\n",
    "    MSE_test = mean_squared_error(y_test, y_predicted)\n",
    "    \n",
    "    return MSE_train, MSE_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T08:40:08.501047500Z",
     "start_time": "2023-09-08T08:40:08.497443200Z"
    }
   },
   "id": "962b6cf10c94c121"
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "outputs": [],
   "source": [
    "def make_dummy_design_matrix(x, degree):\n",
    "    # Initialize an empty list to store the polynomial features\n",
    "    poly_features = []\n",
    "    \n",
    "    # Add the column of ones for the intercept term\n",
    "    poly_features.append(np.ones_like(x))\n",
    "    \n",
    "    # Add polynomial features for each degree up to 'degree'\n",
    "    for d in range(1, degree+1):\n",
    "        poly_features.append(x**d)\n",
    "    \n",
    "    # Stack the polynomial features horizontally to create the design matrix\n",
    "    X = np.hstack(poly_features)\n",
    "    \n",
    "    return X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T08:40:11.896561200Z",
     "start_time": "2023-09-08T08:40:11.887732900Z"
    }
   },
   "id": "d867fd4ce7053076"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate dummy data as in week 35"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aee97288672944b4"
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "outputs": [],
   "source": [
    "def generate_dummy_data(n):\n",
    "    \n",
    "    # Make data set.\n",
    "    x = np.linspace(-3, 3, n).reshape(-1, 1)\n",
    "    y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2)+ np.random.normal(0, 0.1, x.shape)\n",
    "\n",
    "    return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T08:40:15.104311400Z",
     "start_time": "2023-09-08T08:40:15.097304Z"
    }
   },
   "id": "f58f002796997060"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Investigating the role of $\\lambda$ we see that $\\lambda = 0$ gives us the normal regression. Therefor we can just feed $\\lambda = [0, 0.0001,0.001, 0.01,0.1,1.0]$ and do all the regression."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ab54b80f9066527"
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "outputs": [],
   "source": [
    "def generate_df(x, y):\n",
    "    lambda_values = [0, 0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "    poly_orders = [5, 10, 15]\n",
    "    \n",
    "    df = pd.DataFrame({'lambda': lambda_values})\n",
    "    \n",
    "    for order in poly_orders:\n",
    "        MSE = {'Trained': [],\n",
    "               'Test': []}\n",
    "    \n",
    "        # The design matrix\n",
    "        X = make_dummy_design_matrix(x, order)\n",
    "        \n",
    "        for la in lambda_values:\n",
    "            MSE_trained, MSE_test = regression_and_cost_calc(X, y, 1, la)\n",
    "                \n",
    "            MSE['Trained'].append(MSE_trained)\n",
    "            MSE['Test'].append(MSE_test)\n",
    "        \n",
    "        df[f'Trained {order}'] = MSE['Trained']\n",
    "        df[f'Test {order}'] = MSE['Test']\n",
    "    \n",
    "    return df, poly_orders"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T08:40:20.097716100Z",
     "start_time": "2023-09-08T08:40:20.091417400Z"
    }
   },
   "id": "32d677604737232f"
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "outputs": [],
   "source": [
    "def plot(df, poly_orders, run_number):\n",
    "    # Create a colormap with as many colors as there are rows in the DataFrame\n",
    "    colors = ['gray', '#00FFFF', '#FF0000', '#00FF00', '#0000FF', '#FFFF00']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(8,5.3), sharey=True, sharex=True)    \n",
    "    for (index, la), ax in zip(df.iterrows(), axes.ravel()):\n",
    "                \n",
    "        la_val = la['lambda'] \n",
    "        la_str = fr'$\\lambda$={la_val}' if la_val != 0.0000 else 'OLS'\n",
    "      \n",
    "        ax.plot(poly_orders, la.filter(like='Trained'),\n",
    "                color=colors[index])\n",
    "        ax.plot(poly_orders, la.filter(like='Test'),\n",
    "                linestyle='dashed', color=colors[index])\n",
    "        ax.set_title(la_str, \n",
    "                     fontsize=10)\n",
    "        \n",
    "        # Add xlabel and ylabel to each main axis (subplot)\n",
    "        if index >= 3:  # Only add labels to the bottom row of subplots\n",
    "            ax.set_xlabel('Polynomial Order')\n",
    "        if index % 3 == 0:  # Add ylabel only to the leftmost subplots in each row\n",
    "            ax.set_ylabel('Mean Squared Error (MSE)')\n",
    "            \n",
    "    # Visuelle ting:\n",
    "    \n",
    "    plt.tight_layout() # Mindre mellomrom mellom plot\n",
    "    \n",
    "    plt.xticks(poly_orders) # Bare xticks på de aktuelle verdiene for polynomordene\n",
    "    plt.xlim((4, 16)) # Litt mere mellomrom på hver side langs x aksen\n",
    "    \n",
    "    \n",
    "    # Legends\n",
    "    legend_labels = ['Trained', 'Test']\n",
    "    legend_handles = [plt.Line2D([0], [0], color='black', lw=2, label=legend_labels[0]),  # Solid line in black for 'Trained'\n",
    "                      plt.Line2D([0], [0], color='black', lw=2, linestyle='--', label=legend_labels[1])]  # Dashed line in black for 'Test'\n",
    "    plt.legend(handles=legend_handles, labels=legend_labels, loc='upper center', bbox_to_anchor=(-.6, -0.3), ncol=2)\n",
    "    \n",
    "    plt.show()\n",
    "    save_fig(f'Ridge regression, run {run_number}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T08:40:21.277579700Z",
     "start_time": "2023-09-08T08:40:21.274064800Z"
    }
   },
   "id": "b208c213a51bde15"
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "outputs": [],
   "source": [
    "def run(iterations):\n",
    "    for _ in range(iterations):\n",
    "        x, y = generate_dummy_data(100)\n",
    "        df, poly_orders = generate_df(x, y)\n",
    "        plot(df, poly_orders, +1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T08:40:22.299864700Z",
     "start_time": "2023-09-08T08:40:22.291000800Z"
    }
   },
   "id": "ab70cbc272f64cd"
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLinAlgError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[310], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m run(\u001B[38;5;241m2\u001B[39m)\n",
      "Cell \u001B[1;32mIn[309], line 4\u001B[0m, in \u001B[0;36mrun\u001B[1;34m(iterations)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(iterations):\n\u001B[0;32m      3\u001B[0m     x, y \u001B[38;5;241m=\u001B[39m generate_dummy_data(\u001B[38;5;241m100\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m     df, poly_orders \u001B[38;5;241m=\u001B[39m generate_df(x, y)\n\u001B[0;32m      5\u001B[0m     plot(df, poly_orders, \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "Cell \u001B[1;32mIn[307], line 15\u001B[0m, in \u001B[0;36mgenerate_df\u001B[1;34m(x, y)\u001B[0m\n\u001B[0;32m     12\u001B[0m X \u001B[38;5;241m=\u001B[39m make_dummy_design_matrix(x, order)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m la \u001B[38;5;129;01min\u001B[39;00m lambda_values:\n\u001B[1;32m---> 15\u001B[0m     MSE_trained, MSE_test \u001B[38;5;241m=\u001B[39m regression_and_cost_calc(X, y, \u001B[38;5;241m1\u001B[39m, la)\n\u001B[0;32m     17\u001B[0m     MSE[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTrained\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(MSE_trained)\n\u001B[0;32m     18\u001B[0m     MSE[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTest\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(MSE_test)\n",
      "Cell \u001B[1;32mIn[304], line 15\u001B[0m, in \u001B[0;36mregression_and_cost_calc\u001B[1;34m(X, y, methode, la)\u001B[0m\n\u001B[0;32m     12\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTo apply ridge regression provide a numerical values for lambda.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     14\u001B[0m     XtX \u001B[38;5;241m=\u001B[39m X_train\u001B[38;5;241m.\u001B[39mT \u001B[38;5;241m@\u001B[39m X_train\n\u001B[1;32m---> 15\u001B[0m     beta \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39minv(XtX \u001B[38;5;241m+\u001B[39m la\u001B[38;5;241m*\u001B[39mnp\u001B[38;5;241m.\u001B[39midentity(np\u001B[38;5;241m.\u001B[39mshape(XtX)[\u001B[38;5;241m0\u001B[39m])) \u001B[38;5;241m@\u001B[39m (X_train\u001B[38;5;241m.\u001B[39mT \u001B[38;5;241m@\u001B[39m y_train)\n\u001B[0;32m     17\u001B[0m y_tilde \u001B[38;5;241m=\u001B[39m (X_train \u001B[38;5;241m@\u001B[39m beta) \u001B[38;5;66;03m# y_train og x_train er skalert, siden y_tilde kun brukes videre til MSE_train med y_train trenger ikke denne skalering \u001B[39;00m\n\u001B[0;32m     18\u001B[0m y_predicted \u001B[38;5;241m=\u001B[39m (X_test \u001B[38;5;241m@\u001B[39m beta) \u001B[38;5;241m+\u001B[39m y_train_mean \u001B[38;5;66;03m# y_predicted skal brukes til MSE_test med y_test som IKKE er skalert, vi må derfor skalere y_predicted tilbake.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Python\\envs\\DataAnalysis\\Lib\\site-packages\\numpy\\linalg\\linalg.py:561\u001B[0m, in \u001B[0;36minv\u001B[1;34m(a)\u001B[0m\n\u001B[0;32m    559\u001B[0m signature \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mD->D\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m isComplexType(t) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124md->d\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    560\u001B[0m extobj \u001B[38;5;241m=\u001B[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001B[1;32m--> 561\u001B[0m ainv \u001B[38;5;241m=\u001B[39m _umath_linalg\u001B[38;5;241m.\u001B[39minv(a, signature\u001B[38;5;241m=\u001B[39msignature, extobj\u001B[38;5;241m=\u001B[39mextobj)\n\u001B[0;32m    562\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrap(ainv\u001B[38;5;241m.\u001B[39mastype(result_t, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m))\n",
      "File \u001B[1;32mD:\\Python\\envs\\DataAnalysis\\Lib\\site-packages\\numpy\\linalg\\linalg.py:112\u001B[0m, in \u001B[0;36m_raise_linalgerror_singular\u001B[1;34m(err, flag)\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_raise_linalgerror_singular\u001B[39m(err, flag):\n\u001B[1;32m--> 112\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m LinAlgError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSingular matrix\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mLinAlgError\u001B[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "run(2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T08:40:23.520878100Z",
     "start_time": "2023-09-08T08:40:23.459903900Z"
    }
   },
   "id": "e61ac5c04e297b51"
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems that an order of 15 leads to overfitting in most cases (Increased MSE with higher order for the test data). In a choice of $\\lambda$ it is hard to see any definete patterns in the optimal choice as it vary from run to run. Further, as I re-run for new random sets it seems that the normal regression (i.e. $\\lambda = 0$) is performing better having lower MSE for the test prediction then the ridge regression. This is in line with discussion from the teaching hours. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29a1155ca5969a1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
