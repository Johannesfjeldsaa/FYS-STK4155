{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ec219f",
   "metadata": {
    "editable": true
   },
   "source": [
    "<!-- HTML file automatically generated from DocOnce source (https://github.com/doconce/doconce/)\n",
    "doconce format html codeexamplesscaling.do.txt  -->\n",
    "<!-- dom:TITLE: Scaling examples with own code and the library Scikit-Learn -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5df87ed",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Scaling examples with own code and the library Scikit-Learn\n",
    "**Morten Hjorth-Jensen**, Department of Physics, University of Oslo and Department of Physics and Astronomy and Facility for Rare Isotope Beams, Michigan State University\n",
    "\n",
    "Date: **Sep 11, 2023**\n",
    "\n",
    "Copyright 1999-2023, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d166ea",
   "metadata": {
    "editable": true
   },
   "source": [
    "## This note contains code examples with a simple scaling\n",
    "\n",
    "The programs here use both ordinrary least squares (OLS) and Ridge\n",
    "regression with one value only for the hyperparameter $\\lambda$. The\n",
    "first example has no scaling and includes the intercept as well and we\n",
    "are trying to fit a second-order polynomial. The second code takes out\n",
    "the intercept and subtracts the mean values of each column of the\n",
    "design matrix and the mean value of the outputs.\n",
    "\n",
    "The third and final code uses **Scikit-Learn** as library in order to\n",
    "calculate the optimal parameters for OLS and Ridge regression. Note\n",
    "that it is highly recommended to not include the intercept in Ridge\n",
    "and Lasso regression, in order to avoid penalizing the optimization by\n",
    "the intercept. The second and third codes do thus not include the\n",
    "intercept. In the second code we do the scaling ourselves while the\n",
    "last code uses the standard scaler option included in **Scikit-Learn**, known as centering (where\n",
    "we subtract the mean values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef9ae09",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "\n",
    "def OLS_fit_beta(X, y):\n",
    "    return np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "\n",
    "def Ridge_fit_beta(X, y,L,d):\n",
    "    I = np.eye(d,d)\n",
    "    return np.linalg.pinv(X.T @ X + L*I) @ X.T @ y\n",
    "\n",
    "# Same random numbers for each test.\n",
    "np.random.seed(2018)\n",
    "n = 100\n",
    "d = 3\n",
    "# hyperparameter lambda\n",
    "Lambda = 0.01\n",
    "\n",
    "# Make data set, simple second-order polynomial\n",
    "x = np.linspace(-3, 3, n)\n",
    "y = 2.0 + 0.5*x + 5.0*(x**2)+ np.random.randn(n)\n",
    "\n",
    "# The design matrix X includes the intercept and no scaling is made\n",
    "X = np.zeros((len(x), d))\n",
    "for p in range(d):     \n",
    "    X[:, p] = x ** (p) \n",
    "\n",
    "\n",
    "#Split data, no scaling is used and we include the intercept\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "#Calculate beta, own code\n",
    "beta_OLS = OLS_fit_beta(X_train, y_train)\n",
    "beta_Ridge = Ridge_fit_beta(X_train, y_train,Lambda,d)\n",
    "print(beta_OLS)\n",
    "print(beta_Ridge)\n",
    "#predict value\n",
    "ytilde_test_OLS = X_test @ beta_OLS\n",
    "ytilde_test_Ridge = X_test @ beta_Ridge\n",
    "\n",
    "#Calculate MSE\n",
    "print(\"  \")\n",
    "print(\"test MSE of OLS:\")\n",
    "print(MSE(y_test,ytilde_test_OLS))\n",
    "print(\"  \")\n",
    "print(\"test MSE of Ridge\")\n",
    "print(MSE(y_test,ytilde_test_Ridge))\n",
    "\n",
    "plt.scatter(x,y,label='Data')\n",
    "plt.plot(x, X @ beta_OLS,'*', label=\"OLS_Fit\")\n",
    "plt.plot(x, X @ beta_Ridge, label=\"Ridge_Fit\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c368bb8d",
   "metadata": {
    "editable": true
   },
   "source": [
    "In this example we do not include the intercept and we scale the data by subtracting the mean values. This follows the discussion in the [lecture material](https://compphysics.github.io/MachineLearning/doc/LectureNotes/_build/html/chapter3.html#more-on-rescaling-data).\n",
    "see also the weekly slides [for week 36](https://compphysics.github.io/MachineLearning/doc/pub/week36/html/._week36-bs029.html).\n",
    "It is recommended whrn we use Ridge and Lasso regression to not include the intercept in the optimization process.\n",
    "\n",
    "Before we discuss the code, we repeat some of the basic math from the slides of week 36.\n",
    "\n",
    "Let us try to understand what this may imply mathematically when we\n",
    "subtract the mean values, also known as *zero centering* or simply *centering*. For\n",
    "simplicity, we will focus on  ordinary regression, as done in the above example.\n",
    "\n",
    "The cost/loss function  for regression is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba5c048",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\beta_0, \\beta_1, ... , \\beta_{p-1}) = \\frac{1}{n}\\sum_{i=0}^{n} \\left(y_i - \\beta_0 - \\sum_{j=1}^{p-1} X_{ij}\\beta_j\\right)^2,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be61f447",
   "metadata": {
    "editable": true
   },
   "source": [
    "Recall also that we use the squared value. This expression can lead to an\n",
    "increased penalty for higher differences between predicted and\n",
    "output/target values.\n",
    "\n",
    "What we have done is to single out the $\\beta_0$ term in the\n",
    "definition of the mean squared error (MSE).  The design matrix $X$\n",
    "does in this case not contain any intercept column.  When we take the\n",
    "derivative with respect to $\\beta_0$, we want the derivative to obey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efa5b58",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial \\beta_j} = 0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3b54c0",
   "metadata": {
    "editable": true
   },
   "source": [
    "for all $j$. For $\\beta_0$ we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a141a26",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\frac{\\partial C}{\\partial \\beta_0} = -\\frac{2}{n}\\sum_{i=0}^{n-1} \\left(y_i - \\beta_0 - \\sum_{j=1}^{p-1} X_{ij} \\beta_j\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2a9db1",
   "metadata": {
    "editable": true
   },
   "source": [
    "Multiplying away the constant $2/n$, we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279815d",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\sum_{i=0}^{n-1} \\beta_0 = \\sum_{i=0}^{n-1}y_i - \\sum_{i=0}^{n-1} \\sum_{j=1}^{p-1} X_{ij} \\beta_j.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77462b8",
   "metadata": {
    "editable": true
   },
   "source": [
    "Let us specialize first to the case where we have only two parameters $\\beta_0$ and $\\beta_1$.\n",
    "Our result for $\\beta_0$ simplifies then to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef3d129",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "n\\beta_0 = \\sum_{i=0}^{n-1}y_i - \\sum_{i=0}^{n-1} X_{i1} \\beta_1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ca1b4a",
   "metadata": {
    "editable": true
   },
   "source": [
    "We obtain then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6859a8d9",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\beta_0 = \\frac{1}{n}\\sum_{i=0}^{n-1}y_i - \\beta_1\\frac{1}{n}\\sum_{i=0}^{n-1} X_{i1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a018b414",
   "metadata": {
    "editable": true
   },
   "source": [
    "If we define"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3fd8e8",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mu_{\\boldsymbol{x}_1}=\\frac{1}{n}\\sum_{i=0}^{n-1} X_{i1},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1038ff",
   "metadata": {
    "editable": true
   },
   "source": [
    "and the mean value of the outputs as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c979cecf",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mu_y=\\frac{1}{n}\\sum_{i=0}^{n-1}y_i,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1380fd5d",
   "metadata": {
    "editable": true
   },
   "source": [
    "we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e7054e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\beta_0 = \\mu_y - \\beta_1\\mu_{\\boldsymbol{x}_1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f785b8c5",
   "metadata": {
    "editable": true
   },
   "source": [
    "In the general case with more parameters than $\\beta_0$ and $\\beta_1$, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b94b3e7",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\beta_0 = \\frac{1}{n}\\sum_{i=0}^{n-1}y_i - \\frac{1}{n}\\sum_{i=0}^{n-1}\\sum_{j=1}^{p-1} X_{ij}\\beta_j.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1346bbb5",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can rewrite the latter equation as"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044dc319",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\beta_0 = \\frac{1}{n}\\sum_{i=0}^{n-1}y_i - \\sum_{j=1}^{p-1} \\mu_{\\boldsymbol{x}_j}\\beta_j,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae565fb",
   "metadata": {
    "editable": true
   },
   "source": [
    "where we have defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe619e9",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\mu_{\\boldsymbol{x}_j}=\\frac{1}{n}\\sum_{i=0}^{n-1} X_{ij},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025a9267",
   "metadata": {
    "editable": true
   },
   "source": [
    "the mean value for all elements of the column vector $\\boldsymbol{x}_j$.\n",
    "\n",
    "Replacing $y_i$ with $y_i - y_i - \\overline{\\boldsymbol{y}}$ and centering also our design matrix results in a cost function (in vector-matrix disguise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22131558",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "C(\\boldsymbol{\\beta}) = (\\boldsymbol{\\tilde{y}} - \\tilde{X}\\boldsymbol{\\beta})^T(\\boldsymbol{\\tilde{y}} - \\tilde{X}\\boldsymbol{\\beta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3ab42d",
   "metadata": {
    "editable": true
   },
   "source": [
    "If we minimize with respect to $\\boldsymbol{\\beta}$ we have then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca424c8e",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}} = (\\tilde{X}^T\\tilde{X})^{-1}\\tilde{X}^T\\boldsymbol{\\tilde{y}},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2367454f",
   "metadata": {
    "editable": true
   },
   "source": [
    "where $\\boldsymbol{\\tilde{y}} = \\boldsymbol{y} - \\overline{\\boldsymbol{y}}$\n",
    "and $\\tilde{X}_{ij} = X_{ij} - \\frac{1}{n}\\sum_{k=0}^{n-1}X_{kj}$.\n",
    "\n",
    "For Ridge regression we need to add $\\lambda \\boldsymbol{\\beta}^T\\boldsymbol{\\beta}$ to the cost function and get then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf181e1",
   "metadata": {
    "editable": true
   },
   "source": [
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}} = (\\tilde{X}^T\\tilde{X} + \\lambda I)^{-1}\\tilde{X}^T\\boldsymbol{\\tilde{y}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20efe04",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now we try to implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b3cfc82",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(2018)\n",
    "n = 100\n",
    "# we do not include the intercept\n",
    "d = 2\n",
    "Lambda = 0.01\n",
    "\n",
    "# Make data set.\n",
    "x = np.linspace(-3, 3, n)\n",
    "y = 2.0 + 0.5*x + 5.0*(x**2)+ np.random.randn(n)\n",
    "\n",
    "#Design matrix X does not include the intercept. \n",
    "X = np.zeros((len(x), d))\n",
    "for p in range(d):     \n",
    "    X[:, p] = x ** (p+1)\n",
    "\n",
    "\n",
    "#Split data in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Scale data by subtracting mean value,own implementation\n",
    "#For our own implementation, we will need to deal with the intercept by centering the design matrix and the target variable\n",
    "X_train_mean = np.mean(X_train,axis=0)\n",
    "#Center by removing mean from each feature\n",
    "X_train_scaled = X_train - X_train_mean\n",
    "X_test_scaled = X_test - X_train_mean\n",
    "#The model intercept (called y_scaler) is given by the mean of the target variable (IF X is centered, note)\n",
    "y_scaler = np.mean(y_train)\n",
    "y_train_scaled = y_train - y_scaler\n",
    "\n",
    "\n",
    "#Calculate beta\n",
    "beta_OLS = OLS_fit_beta(X_train_scaled, y_train_scaled)\n",
    "beta_Ridge = Ridge_fit_beta(X_train_scaled, y_train_scaled,Lambda,d)\n",
    "print(beta_OLS)\n",
    "print(beta_Ridge)\n",
    "# calculate intercepts and print them\n",
    "interceptOLS = y_scaler - X_train_mean @ beta_OLS\n",
    "interceptRidge = y_scaler - X_train_mean @ beta_Ridge\n",
    "print(interceptOLS)\n",
    "print(interceptRidge)\n",
    "\n",
    "#predict value with intercept\n",
    "ytilde_test_OLS = X_test_scaled @ beta_OLS+y_scaler\n",
    "ytilde_test_Ridge = X_test_scaled @ beta_Ridge+y_scaler\n",
    "\n",
    "\n",
    "#Calculate MSE\n",
    "\n",
    "print(\"  \")\n",
    "print(\"test MSE of OLS:\")\n",
    "print(MSE(y_test,ytilde_test_OLS))\n",
    "print(\"  \")\n",
    "print(\"test MSE of Ridge\")\n",
    "print(MSE(y_test,ytilde_test_Ridge))\n",
    "\n",
    "plt.scatter(x,y,label='Data')\n",
    "plt.plot(x, X @ beta_OLS+interceptOLS,'*', label=\"OLS_Fit\")\n",
    "plt.plot(x, X @ beta_Ridge+interceptRidge, label=\"Ridge_Fit\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f177d5f",
   "metadata": {
    "editable": true
   },
   "source": [
    "Finally, instead of using our own function we repeat the same example\n",
    "using the **standardscaler** functionality of the library\n",
    "**Scikit-Learn**.  Here we limit ourselves to Ridge regression only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9130dfd9",
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "np.random.seed(2018)\n",
    "n = 100\n",
    "d = 2\n",
    "Lambda = 0.01\n",
    "\n",
    "# Make data set.\n",
    "x = np.linspace(-3, 3, n)\n",
    "y = 2.0 + 0.5*x + 5.0*(x**2)+ np.random.randn(n)\n",
    "\n",
    "# Design matrix X does not include the intercept. \n",
    "X = np.zeros((n, d))\n",
    "for p in range(d):     \n",
    "    X[:, p] = x ** (p+1)\n",
    "\n",
    "#Split data in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Scale data by subtracting mean value using scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#Calculate beta\n",
    "OLS = LinearRegression()\n",
    "OLS.fit(X_train,y_train)\n",
    "ypredictOLS = OLS.predict(X_test)\n",
    "RegRidge = linear_model.Ridge(Lambda)\n",
    "RegRidge.fit(X_train,y_train)\n",
    "ypredictRidge = RegRidge.predict(X_test)\n",
    "print(OLS.coef_)\n",
    "print(RegRidge.coef_)\n",
    "print(OLS.intercept_)\n",
    "interceptRidge = RegRidge.intercept_\n",
    "print(RegRidge.intercept_)\n",
    "#predict value without intercept\n",
    "ytilde_test_Ridge = X_test @ RegRidge.coef_+ RegRidge.intercept_\n",
    "ytilde_test_OLS = X_test @ OLS.coef_+ OLS.intercept_\n",
    "\n",
    "#Calculate MSE\n",
    "print(\"  \")\n",
    "print(\"test MSE of OLS\")\n",
    "print(MSE(y_test,ytilde_test_OLS))\n",
    "print(\"  \")\n",
    "print(\"test MSE of Ridge\")\n",
    "print(MSE(y_test,ytilde_test_Ridge))\n",
    "plt.scatter(x,y,label='Data')\n",
    "plt.plot(x, X @ RegRidge.coef_ + RegRidge.intercept_ , label=\"Ridge_Fit\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
